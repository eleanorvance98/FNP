{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab93c262",
   "metadata": {},
   "source": [
    "Learning script for logistic regression classifier for document inclusion criteria\n",
    "1/15/26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d105a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "SIMPLE LEARNING VERSION (Python)\n",
    "TF-IDF + Logistic Regression screening for LexisNexis .docx exports\n",
    "\n",
    "Expected files:\n",
    "- input/nexis_export_translated.docx\n",
    "- input/labels.csv     columns: article_id,label  where label is include/exclude/blank\n",
    "\n",
    "Outputs:\n",
    "- output/screened_ranked_all_articles.csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ac860",
   "metadata": {},
   "source": [
    "OVERVIEW + FOLDER EXPECTATIONS FOR END TO END FORMATTING\n",
    "This notebook expects folder structure to look like:\n",
    "\n",
    "fnp_project_directory/\n",
    "    fnp_vscode/  (this notebook lives here)\n",
    "    input/\n",
    "        nexis_export_translated.docx\n",
    "        input_labels.csv\n",
    "    output/\n",
    "\n",
    "This notebook will:\n",
    "1. read the docx\n",
    "2. split into articles using the line \"LexisNexis\" as an article start marker\n",
    "3. read your labels in the csv\n",
    "4. train a TF-IDF + logistic regression classifier on labeled rows\n",
    "5. score all articles and write output to folder output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db28543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impport libraries for paths, regex, and data. Imports python-docx to read docx files, scikit-learn to build classifier.\n",
    "#If error returned that package isn't found, run pip install python-docx scikit-learn pandas numpy in terminal. However, shouldn't happen because they are all installed on venv.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from docx import Document\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0843d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/ellievance/Documents/WPI/fnp_project_directory/fnp_vscode\n",
      "Project directory: /Users/ellievance/Documents/WPI/fnp_project_directory\n",
      "Input directory: /Users/ellievance/Documents/WPI/fnp_project_directory/input\n",
      "Output directory: /Users/ellievance/Documents/WPI/fnp_project_directory/output\n",
      "Docx path: /Users/ellievance/Documents/WPI/fnp_project_directory/input/nexis_export_translated.docx\n",
      "Labels path: /Users/ellievance/Documents/WPI/fnp_project_directory/input/input_labels.csv\n"
     ]
    }
   ],
   "source": [
    "#Path safe setup for VS Code structure\n",
    "#Sets current project root (FNP_project_directory) as project directory \n",
    "#Constructs paths to input files and creates output folder if missing, prints paths so you can see what it's using, and stops early with errors if files are missing\n",
    "#If you need to hardcode (not running in VS Code), can run: PROJECT_DIR = Path(\"/Users/ellievance/Documents/fnp_project_directory\")\n",
    "\n",
    "VSCODE_DIR = Path.cwd() #gets current wd as path object\n",
    "\n",
    "#Check to see if running from fnp_vscode folder\n",
    "if VSCODE_DIR.name == \"fnp_vscode\": # if yes, then PROJECT_DIR set to parent directory (fnp_project_directory)\n",
    "    PROJECT_DIR = VSCODE_DIR.parent\n",
    "else: \n",
    "    PROJECT_DIR = VSCODE_DIR #if no, assumes already in the project root, so PROJECT_DIR stays current directory\n",
    "#this allows you to run the script from within fnp_vscode folder or from the parent directory \n",
    "\n",
    "#Construct paths to input and output subdirectories within project direcotry\n",
    "INPUT_DIR = PROJECT_DIR / \"input\" \n",
    "OUTPUT_DIR = PROJECT_DIR / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True) #creates output folder if it doesn't exist \n",
    "\n",
    "#Store input file paths so you can reference them later without repeating the entire path structure\n",
    "IN_DOCX = INPUT_DIR / \"nexis_export_translated.docx\" #creates path object to Lexis document file\n",
    "IN_LABELS = INPUT_DIR / \"input_labels.csv\" #path to csv with in/exclusion labels\n",
    "\n",
    "print(\"Working directory:\", VSCODE_DIR)\n",
    "print(\"Project directory:\", PROJECT_DIR)\n",
    "print(\"Input directory:\", INPUT_DIR)\n",
    "print(\"Output directory:\", OUTPUT_DIR)\n",
    "print(\"Docx path:\", IN_DOCX)\n",
    "print(\"Labels path:\", IN_LABELS)\n",
    "\n",
    "#Double check that files exist\n",
    "if not IN_DOCX.exists():\n",
    "    raise FileNotFoundError(f\"Docx not found: {IN_DOCX}\")\n",
    "if not IN_LABELS.exists():\n",
    "    raise FileNotFoundError(f\"Labels CSV not found: {IN_LABELS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fb046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters read from docx: 158624\n",
      "Preview: \n",
      "User Name: =\n",
      "Date and Time: = 2026-01-12\n",
      "Job Number: = 272569488\n",
      "\n",
      "Documents (39)\n",
      "Client/Matter: -None-\n",
      "Search Terms:\n",
      "Search Type: nat\n",
      "\n",
      "1. Mercosur, that necessary yes from Italy The ideas corner The comments\n",
      "\n",
      "2. Vivaldini and the protection of Made in Brescia products: \"Mercosur should avoid low-qu\n"
     ]
    }
   ],
   "source": [
    "#READ THE DOCX AND CONVERT INTO PLAIN TEXT FOR CLASSIFICATION\n",
    "\n",
    "def read_docx_as_text(docx_path: Path) -> str:\n",
    "   \n",
    "# Reads a .docx file and returns all paragraph text in single string with  new lines\n",
    "    \n",
    "    doc = Document(str(docx_path))\n",
    "    paras = [p.text for p in doc.paragraphs if p.text is not None]\n",
    "    return \"\\n\".join(paras)\n",
    "\n",
    "raw_text = read_docx_as_text(IN_DOCX)\n",
    "\n",
    "#Print total character count and article preview\n",
    "print(\"Characters read from docx:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae90b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles found: 39\n",
      "\n",
      "First article preview:\n",
      " User Name: = Date and Time: = 2026-01-12 Job Number: = 272569488 Documents (39) Client/Matter: -None- Search Terms: Search Type: nat 1. Mercosur, that necessary yes from Italy The ideas corner The comments 2. Vivaldini and the protection of Made in Brescia products: \"Mercosur should avoid low-quality imports.\" 3. the fear of joining 4. the fear of joining 5. Tractors return to the heart of the EU 6. The EU revises its agricultural policy. Fitto: \n"
     ]
    }
   ],
   "source": [
    "#Splitting document containing all articles into separate articles, using the \"end of document\" text as a separator \n",
    "def split_into_articles_by_end_marker(text: str, min_chars: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split LexisNexis-export text into articles by using 'End of Document' as the separator.\n",
    "    Filters out front-matter chunks by requiring 'Load Date:' (a strong sign it's an actual article).\n",
    "    \"\"\"\n",
    "\n",
    "   #Remove footers\n",
    "    text = re.sub(r\"(?m)^\\s*Page\\s*\\d+\\s*(?:of|of\\s*)\\s*\\d+\\s*$\\n?\", \"\", text)\n",
    "\n",
    "    # Split on lines that are exactly \"End of Document\"\n",
    "    # Using MULTILINE so ^ and $ work per-line.\n",
    "    chunks = re.split(r\"(?m)^\\s*End of Document\\s*$\", text)\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for ch in chunks:\n",
    "        ch = ch.strip()\n",
    "        if not ch:\n",
    "            continue\n",
    "\n",
    "        #Fix whitespace so everything is single spaces\n",
    "        ch = re.sub(r\"\\s+\", \" \", ch).strip()\n",
    "\n",
    "        # To keep the table of contents from being processed, filtering out chunks of text that don't contain \"Load date\", since all articles have this\n",
    "        if \"Load Date:\" not in ch:\n",
    "            continue\n",
    "\n",
    "        if len(ch) >= min_chars:\n",
    "            cleaned_chunks.append(ch)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"article_id\": np.arange(1, len(cleaned_chunks) + 1), \"text\": cleaned_chunks}\n",
    "    )\n",
    "\n",
    "articles = split_into_articles_by_end_marker(raw_text, min_chars=500)\n",
    "\n",
    "print(\"Articles found:\", len(articles))\n",
    "print(\"\\nFirst article preview:\\n\", articles.loc[0, \"text\"][:450])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392e3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDEBUGGING SPLITTING SO IT DOESN'T CHUNK TOC WITH FIRST ARTICLE\n",
    "def split_into_articles_by_end_marker(text: str, min_chars: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split Nexis export text into articles using 'End of Document' as the separator.\n",
    "    Removes the 'Documents (N)' table-of-contents block if present.\n",
    "    Filters out front matter by requiring 'Load Date:'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) Remove page header/footer lines like \"Page 63 of 64\"\n",
    "    text = re.sub(r\"(?m)^\\s*Page\\s*\\d+\\s*(?:of)?\\s*\\d*\\s*$\\n?\", \"\", text)\n",
    "\n",
    "    # 1) Remove the Nexis \"Documents (N)\" table of contents block (front matter)\n",
    "    # This block starts at \"Documents (39)\" and contains numbered lines like \"1. ...\", \"2. ...\", etc.\n",
    "    # We remove from \"Documents (N)\" through the end of the numbered list.\n",
    "    text = re.sub(\n",
    "        r\"(?ms)^\\s*Documents\\s*\\(\\s*\\d+\\s*\\)\\s*.*?(?=^\\s*[A-Z].+\\n|\\Z)\",\n",
    "        \"\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # If the above is too aggressive/too weak for a future export, we can tighten it.\n",
    "    # For your current file, it removes the TOC list shown in the preview CSV.\n",
    "\n",
    "    # 2) Split on lines that are exactly \"End of Document\"\n",
    "    chunks = re.split(r\"(?m)^\\s*End of Document\\s*$\", text)\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for ch in chunks:\n",
    "        ch = ch.strip()\n",
    "        if not ch:\n",
    "            continue\n",
    "\n",
    "        # Keep only chunks that look like real articles\n",
    "        if \"Load Date:\" not in ch:\n",
    "            continue\n",
    "\n",
    "        # Normalize whitespace after filtering\n",
    "        ch = re.sub(r\"\\s+\", \" \", ch).strip()\n",
    "\n",
    "        if len(ch) >= min_chars:\n",
    "            cleaned_chunks.append(ch)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"article_id\": np.arange(1, len(cleaned_chunks) + 1), \"text\": cleaned_chunks}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5847bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles found: 39\n",
      "Chunks containing 'Documents (N)' (should be 0): 1\n",
      "First chunk preview:\n",
      " User Name: = Date and Time: = 2026-01-12 Job Number: = 272569488 Documents (39) Client/Matter: -None- Search Terms: Search Type: nat 1. Mercosur, that necessary yes from Italy The ideas corner The comments 2. Vivaldini and the protection of Made in Brescia products: \"Mercosur should avoid low-qualit\n"
     ]
    }
   ],
   "source": [
    "#SANITY CHECK FOR DEBUGGING SPLITTING\n",
    "print(\"Articles found:\", len(articles))\n",
    "\n",
    "# The TOC content should not appear inside any chunk now\n",
    "toc_hits = articles[\"text\"].str.contains(r\"Documents\\s*\\(\\s*\\d+\\s*\\)\", regex=True).sum()\n",
    "print(\"Chunks containing 'Documents (N)' (should be 0):\", toc_hits)\n",
    "\n",
    "print(\"First chunk preview:\\n\", articles.loc[0, \"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd037d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote article index preview to: /Users/ellievance/Documents/WPI/fnp_project_directory/output/article_index_preview.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>User Name: = Date and Time: = 2026-01-12 Job N...</td>\n",
       "      <td>User Name: = Date and Time: = 2026-01-12 Job N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vivaldini and the protection of Made in Bresci...</td>\n",
       "      <td>Vivaldini and the protection of Made in Bresci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>the fear of joining Corriere della Sera (Italy...</td>\n",
       "      <td>the fear of joining Corriere della Sera (Italy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>the fear of joining Corriere della Sera (Italy...</td>\n",
       "      <td>the fear of joining Corriere della Sera (Italy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tractors return to the heart of the EU Corrier...</td>\n",
       "      <td>Tractors return to the heart of the EU Corrier...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                               text  \\\n",
       "0           1  User Name: = Date and Time: = 2026-01-12 Job N...   \n",
       "1           2  Vivaldini and the protection of Made in Bresci...   \n",
       "2           3  the fear of joining Corriere della Sera (Italy...   \n",
       "3           4  the fear of joining Corriere della Sera (Italy...   \n",
       "4           5  Tractors return to the heart of the EU Corrier...   \n",
       "\n",
       "                                             preview  \n",
       "0  User Name: = Date and Time: = 2026-01-12 Job N...  \n",
       "1  Vivaldini and the protection of Made in Bresci...  \n",
       "2  the fear of joining Corriere della Sera (Italy...  \n",
       "3  the fear of joining Corriere della Sera (Italy...  \n",
       "4  Tractors return to the heart of the EU Corrier...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: create an index csv to double check that chunks correspond to correct article IDs\n",
    "index = articles.copy()\n",
    "index[\"preview\"] = index[\"text\"].str[:250] #preview column\n",
    "index_path = OUTPUT_DIR / \"article_index_preview.csv\" #file path in output folder\n",
    "index.to_csv(index_path, index=False) #df written to csv\n",
    "\n",
    "print(\"Wrote article index preview to:\", index_path) #double check where file was written\n",
    "index.head(5) #preview new file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27535001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
